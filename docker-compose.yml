# Определяем "шаблон" для app-сервисов с помощью якоря
x-app-base: &app-base
  build:
    context: .                         # Контекст сборки - корень проекта
    dockerfile: docker/Dockerfile      # Путь к Dockerfile
    target: runner                     # Этап №2 'Runner'
  env_file: .env                       # Передаем все переменные из .env в контейнер
  volumes:
    - media_volume:/app/media          # Монтируем том для медиа
    - logs_volume:/app/logs            # Монтируем том для логов
  depends_on:
    pgbouncer:
      # Запускаем только после того, как pgbouncer станет healthy
      condition: service_healthy
    redis:
      # Запускаем только после того, как redis станет healthy
      condition: service_healthy
  restart: unless-stopped
  networks:
    - backend

services:
  # --- INFRASTRUCTURE ---
  # Сервис базы данных (PostgreSQL)
  # Настроен для работы на сервере с 1 ГБ RAM
  db:
    image: postgres:18.1-bookworm
    container_name: kronon_db
    # Увеличиваем лимит разделяемой памяти (важно для сложных запросов)
    shm_size: 256mb
    # Тонкая настройка производительности через флаги запуска:
    # - shared_buffers: кэш страниц БД (25% от RAM)
    # - effective_cache_size: оценка доступного кэша для планировщика (75% от RAM)
    # - maintenance_work_mem: сервисные операции (индексы, вакуум), важно для скорости миграций
    # - work_mem: лимит памяти на одну операцию сортировки внутри запроса
    # - max_connections: лимит реальных процессов БД (с PgBouncer можно держать низким, экономя RAM)
    # - random_page_cost: оптимизация под SSD (сообщаем, что случайное чтение дешевое)
    # - effective_io_concurrency: использование преимуществ AIO в Postgres 18
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=768MB
      -c maintenance_work_mem=64MB
      -c work_mem=8MB
      -c max_connections=50
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
    environment:
      POSTGRES_DB: ${DB_NAME:-kronon_db}
      POSTGRES_USER: ${DB_USER:-kronon_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data  # Монтируем том для БД
    deploy:
      resources:
        limits:
          cpus: "1.0"                           # Максимум 1 ядро
          memory: 1024M                         # Максимум 1 ГБ RAM
        reservations:
          memory: 256M                          # Гарантированный резерв
    healthcheck:
      # Проверяет, готова ли БД принимать соединения (экранируем $ для shell)
      test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s  # Даем время на запуск Postgres перед первой проверкой
    restart: unless-stopped
    networks:
      - backend

  # Пулер соединений (PgBouncer)
  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: kronon_pgbouncer
    environment:
      # Хост и ПОРТ реальной БД
      - DB_HOST=${POSTGRES_HOST:-db}
      - DB_PORT=${POSTGRES_PORT:-5432}
      # Порт, который будет слушать сам PgBouncer
      - LISTEN_PORT=6432
      # Доступ к БД
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - ADMIN_USERS=${DB_USER}
      # Настройки пулинга
      - POOL_MODE=transaction        # Самый эффективный режим для Django/Celery
      - MAX_CLIENT_CONN=100          # Сколько приложений может подключиться к пулеру
      - DEFAULT_POOL_SIZE=20         # Сколько реальных соединений держать открытыми к БД
    depends_on:
      db:
        # Запускаем только после того, как db станет healthy
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -h localhost -p 6432 -U $${DB_USER} -d $${DB_NAME}" ]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped
    networks:
      - backend

  # Кэш и брокер сообщений (Redis)
  redis:
    image: redis:7-alpine
    container_name: kronon_redis
    # Команда для персистентности (сохранять дамп БД каждые 60 сек, если было хотя бы 1 изменение)
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis_data:/data  # Монтируем том для сохранения dump.rdb
    deploy:
      resources:
        limits:
          memory: 128M  #  Максимум 128 МБ RAM
    healthcheck:
      # Проверяет, отвечает ли Redis на команду PING
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s  # Даем время на запуск Redis перед первой проверкой
    restart: unless-stopped
    networks:
      - backend

  # --- APPLICATION ---
  app:
    <<: *app-base
    container_name: kronon_web
    # Команда перехватывается в Entrypoint и модифицируется
    command: gunicorn
    volumes:
        - static_volume:/app/staticfiles  # Монтируем том для статики
    healthcheck:
        # Временный healthcheck, пока нет специального эндпоинта /health
        test: [ "CMD", "curl", "-f", "http://localhost:8000/admin/login/" ]
        interval: 15s
        timeout: 5s
        retries: 3
        start_period: 20s  # Даем время на запуск app перед первой проверкой

  # Celery Worker
  celery_worker:
      <<: *app-base
      container_name: kronon_celery
      command: celery -A config.celery:app worker -l info -c 2

# --- MONITORING ---
#  prometheus:
#      image: prom/prometheus:latest
#      volumes:
#          - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
#          - prometheus_data:/prometheus
#      ports:
#          - "9090:9090"
#      networks:
#          - backend
#
#  loki:
#      image: grafana/loki:latest
#      volumes:
#          - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml:ro
#          - loki_data:/loki
#      ports:
#          - "3100:3100"
#      networks:
#          - backend
#
#  promtail:
#      image: grafana/promtail:latest
#      volumes:
#          - ./monitoring/promtail-config.yaml:/etc/promtail/config.yaml:ro
#          - /var/lib/docker/containers:/var/lib/docker/containers:ro
#          - /var/run/docker.sock:/var/run/docker.sock
#      networks:
#          - backend
#
#  grafana:
#      image: grafana/grafana:latest
#      volumes:
#          - grafana_data:/var/lib/grafana
#      environment:
#          GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
#          GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
#      ports:
#          - "3000:3000"
#      depends_on:
#          - prometheus
#          - loki
#      networks:
#          - backend
#
#    celery-exporter:
#      image: danihodovic/celery-exporter:latest
#      container_name: monitoring_celery_exporter
#      command: --broker-url=${CELERY_BROKER_URL:-redis://redis:6379/2}
#      restart: unless-stopped
#      networks:
#        - backend

# --- STORAGE & NETWORKING ---
# Определяем все тома для хранения персистентных данных
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  logs_volume:
      driver: local
  media_volume:
      driver: local
  static_volume:
      driver: local

# Определяем общую сеть для всех сервисов
# Это обеспечивает изоляцию и позволяет контейнерам общаться друг с другом по именам
networks:
  backend:
    driver: bridge
